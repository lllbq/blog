---
layout: post
title:  "梯度下降法"
subtitle: '梯度下降法'
date:   2019-10-03 14:53
tags: 机器学习
description: '梯度下降算法理论及代码调用'
color: 'rgb(230,230,250)'
cover: '/../assets/picture/caicai33.jpg'
---

**梯度**

对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数$f(x,y)$，分别对$x,y$求偏导数，求得的梯度向量就是$(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})^T$，简称$grad\space f(x,y)$或者$\triangledown f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0})^T$或者$\triangledown f(x_0,y_0)$，如果是3个参数的向量梯度，就是$(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z})^T$，以此类推。

梯度向量从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数$f(x,y)$，在点$(x_0,y_0)$，沿着梯度向量的方向就是$(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0})^T$的方向，是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$-(\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial y_0})^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。

**梯度下降的相关概念**

**步长(learning rate): **又称**学习率**，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。

**损失函数(loss function):** 为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。

**梯度下降法调优**

**步长:** 步长太小，收敛慢，步长太大，会远离最优解。

**初始值:** 随机选取初始值，当损失函数是非凸函数时，找到的解可能是局部最优解，需要多测试几次，从局部最优解中选出最优解。当损失函数是凸函数时，得到的解就是最优解。

**归一化:** 如果不归一化，会收敛的很慢，会形成之字的路线。

**梯度下降法分类**

**批量梯度下降法(BGD)**

计算梯度时使用所有的样本，这样每次算出来的梯度都是当前最优的方向。

- 优点

1. 迭代次数少
2. 若损失函数为凸函数，能够保证收敛到全局最优解；若为非凸函数，能够收敛到局部最优值（结果的准确度）

- 缺点

1. 训练速度慢（时间，每一次训练需要的时间）
2. 需要内存大（空间）
3. 不支持在线更新

**随机梯度下降法(SGD)**

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本数据，而是仅仅选取一个样本来求梯度。

- 优点

1. 训练速度快
2. 支持在线更新
3. 有几率跳出局部最优解

- 缺点

1. 容易收敛到局部最优，并且容易被困在鞍点
2. 迭代次数多

**小批量梯度下降法(MBGD)**

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用$x$个样子来迭代，$1<x<m$。

**scikit-learning中的随机梯度下降法**

```python
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.linear_model import SGDRegressor

# 加载波士顿房价数据集
boston = datasets.load_boston()
X = boston.data
y = boston.target

# 数据归一化
standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)

# 数据集分割
X_train, X_test, y_train, y_test = train_test_split(X_standard, y, random_state=666)

# 构建随机梯度下降模型
sgd_reg = SGDRegressor(loss='squared_loss', average=True, penalty='l2')

# 训练模型
sgd_reg.fit(X_train, y_train)

# 测试数据集
print(sgd_reg.score(X_train, y_train))
print(sgd_reg.score(X_test, y_test))
# 0.7465130772789506
# 0.6508146425949382
```

```python
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.linear_model import SGDClassifier

# 加载手写数字数据集
digits = datasets.load_digits()
X = digits.data
y = digits.target

# 数据归一化
standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)

# 数据集分割
X_train, X_test, y_train, y_test = train_test_split(X_standard, y, random_state=666)

# 构建随机梯度下降模型
sgd_reg = SGDClassifier()

# 训练模型
sgd_reg.fit(X_train, y_train)

# 测试数据集
print(sgd_reg.score(X_test, y_test))
# 0.96
```

